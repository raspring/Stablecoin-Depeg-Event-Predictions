{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stablecoin Depeg Prediction - Model Training & Selection\n",
    "\n",
    "This notebook covers the complete ML pipeline:\n",
    "1. Data preparation and feature engineering\n",
    "2. Train/test split (time series aware)\n",
    "3. Baseline models\n",
    "4. Hyperparameter tuning\n",
    "5. Model evaluation and comparison\n",
    "6. Feature importance analysis\n",
    "7. Final model selection and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score, roc_curve,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Project imports\n",
    "import sys\n",
    "PROJECT_ROOT = Path('.').resolve().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from config.settings import PROCESSED_DATA_DIR\n",
    "from src.features.engineering import create_features, create_target\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "colors = {'usdt': '#26A17B', 'usdc': '#2775CA'}\n",
    "\n",
    "# Random seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "CONFIG = {\n",
    "    'threshold': 0.005,      # Depeg threshold (0.5%)\n",
    "    'horizon_days': 7,       # Prediction horizon\n",
    "    'cv_splits': 5,          # Time series CV splits\n",
    "    'use_multi_coin': True,  # Use combined USDT + USDC data\n",
    "}\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "if CONFIG['use_multi_coin']:\n",
    "    df = pd.read_csv(PROCESSED_DATA_DIR / 'combined_stablecoins_daily.csv')\n",
    "    print(f\"Loaded combined data: {len(df):,} rows\")\n",
    "else:\n",
    "    df = pd.read_csv(PROCESSED_DATA_DIR / 'usdt_merged_daily.csv')\n",
    "    df['coin'] = 'usdt'\n",
    "    print(f\"Loaded USDT data: {len(df):,} rows\")\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "print(f\"Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"Coins: {df['coin'].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df, threshold=0.005, horizon_days=7):\n",
    "    \"\"\"Create features and target for modeling.\"\"\"\n",
    "    \n",
    "    all_dfs = []\n",
    "    \n",
    "    for coin in df['coin'].unique():\n",
    "        coin_df = df[df['coin'] == coin].copy()\n",
    "        coin_df = coin_df.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # === Price Features ===\n",
    "        coin_df['btc_return_1d'] = coin_df['close'].pct_change()\n",
    "        coin_df['btc_return_7d'] = coin_df['close'].pct_change(periods=7)\n",
    "        coin_df['btc_return_30d'] = coin_df['close'].pct_change(periods=30)\n",
    "        coin_df['btc_volatility_7d'] = coin_df['btc_return_1d'].rolling(7).std()\n",
    "        coin_df['btc_volatility_30d'] = coin_df['btc_return_1d'].rolling(30).std()\n",
    "        \n",
    "        # BTC drawdown\n",
    "        coin_df['btc_rolling_max_30d'] = coin_df['close'].rolling(30).max()\n",
    "        coin_df['btc_drawdown_30d'] = (coin_df['close'] - coin_df['btc_rolling_max_30d']) / coin_df['btc_rolling_max_30d']\n",
    "        \n",
    "        # === Volume Features ===\n",
    "        coin_df['volume_ma_7d'] = coin_df['quote_volume'].rolling(7).mean()\n",
    "        coin_df['volume_ma_30d'] = coin_df['quote_volume'].rolling(30).mean()\n",
    "        coin_df['volume_ratio_7d'] = coin_df['quote_volume'] / coin_df['volume_ma_7d']\n",
    "        coin_df['volume_ratio_30d'] = coin_df['quote_volume'] / coin_df['volume_ma_30d']\n",
    "        \n",
    "        # === Volatility Features ===\n",
    "        coin_df['spread_ma_7d'] = coin_df['spread_proxy'].rolling(7).mean()\n",
    "        coin_df['spread_ma_30d'] = coin_df['spread_proxy'].rolling(30).mean()\n",
    "        coin_df['spread_zscore'] = (\n",
    "            (coin_df['spread_proxy'] - coin_df['spread_ma_30d']) / \n",
    "            coin_df['spread_proxy'].rolling(30).std()\n",
    "        )\n",
    "        \n",
    "        # === Supply Features ===\n",
    "        coin_df['supply_change_1d'] = coin_df['total_circulating'].pct_change()\n",
    "        coin_df['supply_change_7d'] = coin_df['total_circulating'].pct_change(periods=7)\n",
    "        coin_df['supply_volatility_7d'] = coin_df['supply_change_1d'].rolling(7).std()\n",
    "        \n",
    "        # === Price Deviation Features ===\n",
    "        coin_df['price_deviation'] = coin_df['implied_price'] - 1.0\n",
    "        coin_df['abs_deviation'] = coin_df['price_deviation'].abs()\n",
    "        coin_df['deviation_ma_7d'] = coin_df['price_deviation'].rolling(7).mean()\n",
    "        \n",
    "        # === Interaction Features ===\n",
    "        coin_df['stress_indicator'] = coin_df['spread_zscore'] * coin_df['volume_ratio_7d']\n",
    "        coin_df['flight_to_safety'] = (-coin_df['btc_return_1d']) * coin_df['supply_change_1d'].clip(lower=0)\n",
    "        \n",
    "        # === Target: Will deviation exceed threshold in next N days? ===\n",
    "        coin_df['future_max_deviation'] = (\n",
    "            coin_df['abs_deviation']\n",
    "            .rolling(horizon_days, min_periods=1)\n",
    "            .max()\n",
    "            .shift(-horizon_days)\n",
    "        )\n",
    "        coin_df['target'] = (coin_df['future_max_deviation'] >= threshold).astype(int)\n",
    "        \n",
    "        all_dfs.append(coin_df)\n",
    "    \n",
    "    return pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Create features\n",
    "df_features = prepare_features(df, CONFIG['threshold'], CONFIG['horizon_days'])\n",
    "print(f\"\\nFeatures created: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "FEATURE_COLS = [\n",
    "    # BTC price features\n",
    "    'btc_return_1d', 'btc_return_7d', 'btc_return_30d',\n",
    "    'btc_volatility_7d', 'btc_volatility_30d', 'btc_drawdown_30d',\n",
    "    \n",
    "    # Volume features\n",
    "    'volume_ratio_7d', 'volume_ratio_30d',\n",
    "    \n",
    "    # Volatility features\n",
    "    'spread_proxy', 'spread_ma_7d', 'spread_zscore',\n",
    "    \n",
    "    # Buy pressure\n",
    "    'buy_ratio',\n",
    "    \n",
    "    # Supply features\n",
    "    'supply_change_1d', 'supply_change_7d', 'supply_volatility_7d',\n",
    "    \n",
    "    # Current deviation\n",
    "    'price_deviation', 'abs_deviation',\n",
    "    \n",
    "    # Interaction features\n",
    "    'stress_indicator', 'flight_to_safety',\n",
    "]\n",
    "\n",
    "# Add Fear & Greed if available\n",
    "if 'fear_greed_value' in df_features.columns:\n",
    "    FEATURE_COLS.append('fear_greed_value')\n",
    "\n",
    "# Add coin dummy for multi-coin\n",
    "if CONFIG['use_multi_coin']:\n",
    "    df_features['is_usdc'] = (df_features['coin'] == 'usdc').astype(int)\n",
    "    FEATURE_COLS.append('is_usdc')\n",
    "\n",
    "print(f\"Feature columns ({len(FEATURE_COLS)}):\")\n",
    "print(FEATURE_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data - remove rows with NaN in features or target\n",
    "df_clean = df_features.dropna(subset=FEATURE_COLS + ['target']).copy()\n",
    "\n",
    "# Handle infinite values\n",
    "df_clean = df_clean.replace([np.inf, -np.inf], np.nan).dropna(subset=FEATURE_COLS)\n",
    "\n",
    "X = df_clean[FEATURE_COLS]\n",
    "y = df_clean['target']\n",
    "\n",
    "print(f\"\\nFinal dataset:\")\n",
    "print(f\"  Samples: {len(X):,}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Target distribution:\")\n",
    "print(f\"    Class 0 (no depeg): {(y==0).sum():,} ({(y==0).mean()*100:.1f}%)\")\n",
    "print(f\"    Class 1 (depeg):    {(y==1).sum():,} ({(y==1).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split (last 20% for testing)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# Get dates for reference\n",
    "train_dates = df_clean['date'].iloc[:split_idx]\n",
    "test_dates = df_clean['date'].iloc[split_idx:]\n",
    "\n",
    "print(\"Train/Test Split (Time-based):\")\n",
    "print(f\"  Train: {len(X_train):,} samples ({train_dates.min().date()} to {train_dates.max().date()})\")\n",
    "print(f\"  Test:  {len(X_test):,} samples ({test_dates.min().date()} to {test_dates.max().date()})\")\n",
    "print(f\"\\nTrain target distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Test target distribution:  {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled with StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series cross-validation for training\n",
    "tscv = TimeSeriesSplit(n_splits=CONFIG['cv_splits'])\n",
    "\n",
    "# Visualize CV splits\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "for i, (train_idx, val_idx) in enumerate(tscv.split(X_train_scaled)):\n",
    "    ax.scatter(train_idx, [i]*len(train_idx), c='blue', s=1, label='Train' if i==0 else '')\n",
    "    ax.scatter(val_idx, [i]*len(val_idx), c='red', s=1, label='Validation' if i==0 else '')\n",
    "\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('CV Fold')\n",
    "ax.set_title('Time Series Cross-Validation Splits')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Train model and return evaluation metrics.\"\"\"\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'accuracy': (y_pred == y_test).mean(),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    if y_prob is not None and len(np.unique(y_test)) > 1:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_prob)\n",
    "        metrics['avg_precision'] = average_precision_score(y_test, y_prob)\n",
    "    else:\n",
    "        metrics['roc_auc'] = np.nan\n",
    "        metrics['avg_precision'] = np.nan\n",
    "    \n",
    "    return metrics, model, y_pred, y_prob\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weight\n",
    "class_weight = {0: 1, 1: (y_train == 0).sum() / (y_train == 1).sum()}\n",
    "print(f\"Class weight: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=SEED\n",
    ")\n",
    "metrics, model, y_pred, y_prob = evaluate_model(\n",
    "    lr, X_train_scaled, X_test_scaled, y_train, y_test, 'Logistic Regression'\n",
    ")\n",
    "results.append(metrics)\n",
    "models['lr'] = model\n",
    "print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "metrics, model, y_pred, y_prob = evaluate_model(\n",
    "    rf, X_train_scaled, X_test_scaled, y_train, y_test, 'Random Forest'\n",
    ")\n",
    "results.append(metrics)\n",
    "models['rf'] = model\n",
    "print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Gradient Boosting\n",
    "print(\"Training Gradient Boosting...\")\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "# Use sample weights for class imbalance\n",
    "sample_weight = np.where(y_train == 1, class_weight[1], 1)\n",
    "gb.fit(X_train_scaled, y_train, sample_weight=sample_weight)\n",
    "\n",
    "y_pred = gb.predict(X_test_scaled)\n",
    "y_prob = gb.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "metrics = {\n",
    "    'model': 'Gradient Boosting',\n",
    "    'accuracy': (y_pred == y_test).mean(),\n",
    "    'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'roc_auc': roc_auc_score(y_test, y_prob),\n",
    "    'avg_precision': average_precision_score(y_test, y_prob)\n",
    "}\n",
    "results.append(metrics)\n",
    "models['gb'] = gb\n",
    "print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: SVM\n",
    "print(\"Training SVM...\")\n",
    "svm = SVC(\n",
    "    kernel='rbf',\n",
    "    class_weight='balanced',\n",
    "    probability=True,\n",
    "    random_state=SEED\n",
    ")\n",
    "metrics, model, y_pred, y_prob = evaluate_model(\n",
    "    svm, X_train_scaled, X_test_scaled, y_train, y_test, 'SVM (RBF)'\n",
    ")\n",
    "results.append(metrics)\n",
    "models['svm'] = model\n",
    "print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5: Neural Network\n",
    "print(\"Training Neural Network...\")\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    random_state=SEED\n",
    ")\n",
    "metrics, model, y_pred, y_prob = evaluate_model(\n",
    "    mlp, X_train_scaled, X_test_scaled, y_train, y_test, 'Neural Network'\n",
    ")\n",
    "results.append(metrics)\n",
    "models['mlp'] = model\n",
    "print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "display(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best baseline model for tuning\n",
    "best_baseline = results_df.loc[results_df['roc_auc'].idxmax(), 'model']\n",
    "print(f\"Best baseline model: {best_baseline}\")\n",
    "print(f\"\\nTuning Random Forest and Gradient Boosting...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Random Forest\n",
    "print(\"\\nTuning Random Forest...\")\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [5, 10, 20],\n",
    "    'min_samples_leaf': [2, 5, 10],\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(class_weight='balanced', random_state=SEED, n_jobs=-1),\n",
    "    rf_params,\n",
    "    n_iter=20,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best params: {rf_search.best_params_}\")\n",
    "print(f\"Best CV ROC-AUC: {rf_search.best_score_:.4f}\")\n",
    "\n",
    "models['rf_tuned'] = rf_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Gradient Boosting\n",
    "print(\"\\nTuning Gradient Boosting...\")\n",
    "\n",
    "gb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'min_samples_split': [5, 10, 20],\n",
    "}\n",
    "\n",
    "gb_search = RandomizedSearchCV(\n",
    "    GradientBoostingClassifier(random_state=SEED),\n",
    "    gb_params,\n",
    "    n_iter=20,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gb_search.fit(X_train_scaled, y_train, sample_weight=sample_weight)\n",
    "\n",
    "print(f\"Best params: {gb_search.best_params_}\")\n",
    "print(f\"Best CV ROC-AUC: {gb_search.best_score_:.4f}\")\n",
    "\n",
    "models['gb_tuned'] = gb_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned models on test set\n",
    "tuned_results = []\n",
    "\n",
    "for name, model in [('RF Tuned', models['rf_tuned']), ('GB Tuned', models['gb_tuned'])]:\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'model': name,\n",
    "        'accuracy': (y_pred == y_test).mean(),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_test, y_prob),\n",
    "        'avg_precision': average_precision_score(y_test, y_prob)\n",
    "    }\n",
    "    tuned_results.append(metrics)\n",
    "\n",
    "# Combine all results\n",
    "all_results = pd.concat([results_df, pd.DataFrame(tuned_results)], ignore_index=True)\n",
    "all_results = all_results.sort_values('roc_auc', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL MODELS COMPARISON (sorted by ROC-AUC)\")\n",
    "print(\"=\"*70)\n",
    "display(all_results.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "best_model_name = all_results.iloc[0]['model']\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "\n",
    "# Map to model object\n",
    "model_map = {\n",
    "    'Logistic Regression': 'lr',\n",
    "    'Random Forest': 'rf',\n",
    "    'Gradient Boosting': 'gb',\n",
    "    'SVM (RBF)': 'svm',\n",
    "    'Neural Network': 'mlp',\n",
    "    'RF Tuned': 'rf_tuned',\n",
    "    'GB Tuned': 'gb_tuned'\n",
    "}\n",
    "\n",
    "best_model = models[model_map[best_model_name]]\n",
    "\n",
    "# Get predictions\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "y_prob_best = best_model.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"DETAILED EVALUATION: {best_model_name}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['No Depeg', 'Depeg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "ax1 = axes[0]\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "            xticklabels=['No Depeg', 'Depeg'],\n",
    "            yticklabels=['No Depeg', 'Depeg'])\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "ax1.set_title(f'Confusion Matrix - {best_model_name}')\n",
    "\n",
    "# Normalized confusion matrix\n",
    "ax2 = axes[1]\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues', ax=ax2,\n",
    "            xticklabels=['No Depeg', 'Depeg'],\n",
    "            yticklabels=['No Depeg', 'Depeg'])\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "ax2.set_title('Normalized Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and Precision-Recall curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "ax1 = axes[0]\n",
    "for name, model_key in [('RF Tuned', 'rf_tuned'), ('GB Tuned', 'gb_tuned'), ('Logistic Regression', 'lr')]:\n",
    "    if model_key in models:\n",
    "        y_prob = models[model_key].predict_proba(X_test_scaled)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "        ax1.plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})')\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curves')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "# Precision-Recall Curve\n",
    "ax2 = axes[1]\n",
    "for name, model_key in [('RF Tuned', 'rf_tuned'), ('GB Tuned', 'gb_tuned'), ('Logistic Regression', 'lr')]:\n",
    "    if model_key in models:\n",
    "        y_prob = models[model_key].predict_proba(X_test_scaled)[:, 1]\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "        ap = average_precision_score(y_test, y_prob)\n",
    "        ax2.plot(recall, precision, label=f'{name} (AP={ap:.3f})')\n",
    "\n",
    "ax2.axhline(y=y_test.mean(), color='k', linestyle='--', label=f'Baseline ({y_test.mean():.3f})')\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curves')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold analysis\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "threshold_metrics = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_prob_best >= thresh).astype(int)\n",
    "    threshold_metrics.append({\n",
    "        'threshold': thresh,\n",
    "        'precision': precision_score(y_test, y_pred_thresh, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred_thresh, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    })\n",
    "\n",
    "thresh_df = pd.DataFrame(threshold_metrics)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(thresh_df['threshold'], thresh_df['precision'], 'b-', label='Precision')\n",
    "ax.plot(thresh_df['threshold'], thresh_df['recall'], 'r-', label='Recall')\n",
    "ax.plot(thresh_df['threshold'], thresh_df['f1'], 'g-', label='F1')\n",
    "ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Mark optimal F1 threshold\n",
    "best_thresh_idx = thresh_df['f1'].idxmax()\n",
    "best_thresh = thresh_df.loc[best_thresh_idx, 'threshold']\n",
    "ax.axvline(x=best_thresh, color='green', linestyle=':', label=f'Best F1 @ {best_thresh:.2f}')\n",
    "\n",
    "ax.set_xlabel('Classification Threshold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Precision, Recall, F1 vs Classification Threshold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal threshold for F1: {best_thresh:.2f}\")\n",
    "print(f\"  Precision: {thresh_df.loc[best_thresh_idx, 'precision']:.4f}\")\n",
    "print(f\"  Recall: {thresh_df.loc[best_thresh_idx, 'recall']:.4f}\")\n",
    "print(f\"  F1: {thresh_df.loc[best_thresh_idx, 'f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from tree-based models\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': FEATURE_COLS,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.barh(importance['feature'], importance['importance'], color='steelblue')\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title(f'Feature Importance - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Features:\")\n",
    "    print(importance.tail(10).to_string(index=False))\n",
    "    \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': FEATURE_COLS,\n",
    "        'coefficient': best_model.coef_[0]\n",
    "    }).sort_values('coefficient', key=abs, ascending=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    colors = ['red' if x < 0 else 'green' for x in importance['coefficient']]\n",
    "    ax.barh(importance['feature'], importance['coefficient'], color=colors)\n",
    "    ax.set_xlabel('Coefficient')\n",
    "    ax.set_title(f'Feature Coefficients - {best_model_name}')\n",
    "    ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importance across models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, (name, model_key) in zip(axes, [('Random Forest', 'rf_tuned'), ('Gradient Boosting', 'gb_tuned')]):\n",
    "    if model_key in models and hasattr(models[model_key], 'feature_importances_'):\n",
    "        imp = pd.DataFrame({\n",
    "            'feature': FEATURE_COLS,\n",
    "            'importance': models[model_key].feature_importances_\n",
    "        }).sort_values('importance', ascending=True).tail(10)\n",
    "        \n",
    "        ax.barh(imp['feature'], imp['importance'], color='steelblue')\n",
    "        ax.set_xlabel('Importance')\n",
    "        ax.set_title(f'Top 10 Features - {name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "model_path = MODELS_DIR / 'best_model.joblib'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Saved best model to: {model_path}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = MODELS_DIR / 'scaler.joblib'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Saved scaler to: {scaler_path}\")\n",
    "\n",
    "# Save feature list\n",
    "features_path = MODELS_DIR / 'features.txt'\n",
    "with open(features_path, 'w') as f:\n",
    "    f.write('\\n'.join(FEATURE_COLS))\n",
    "print(f\"Saved feature list to: {features_path}\")\n",
    "\n",
    "# Save model config\n",
    "config_path = MODELS_DIR / 'config.txt'\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(f\"Model: {best_model_name}\\n\")\n",
    "    f.write(f\"Threshold: {CONFIG['threshold']}\\n\")\n",
    "    f.write(f\"Horizon: {CONFIG['horizon_days']} days\\n\")\n",
    "    f.write(f\"Train date range: {train_dates.min().date()} to {train_dates.max().date()}\\n\")\n",
    "    f.write(f\"Test date range: {test_dates.min().date()} to {test_dates.max().date()}\\n\")\n",
    "    f.write(f\"Test ROC-AUC: {all_results.iloc[0]['roc_auc']:.4f}\\n\")\n",
    "print(f\"Saved config to: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "results_path = MODELS_DIR / 'model_comparison.csv'\n",
    "all_results.to_csv(results_path, index=False)\n",
    "print(f\"Saved results to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total samples: {len(X):,}\")\n",
    "print(f\"  Features: {len(FEATURE_COLS)}\")\n",
    "print(f\"  Positive class rate: {y.mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Depeg threshold: {CONFIG['threshold']*100:.1f}%\")\n",
    "print(f\"  Prediction horizon: {CONFIG['horizon_days']} days\")\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "best_metrics = all_results.iloc[0]\n",
    "print(f\"  ROC-AUC: {best_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall: {best_metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score: {best_metrics['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  {model_path}\")\n",
    "print(f\"  {scaler_path}\")\n",
    "print(f\"  {features_path}\")\n",
    "print(f\"  {config_path}\")\n",
    "print(f\"  {results_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Load and Use Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load and use saved model\n",
    "# Uncomment to run\n",
    "\n",
    "# # Load model and scaler\n",
    "# loaded_model = joblib.load(MODELS_DIR / 'best_model.joblib')\n",
    "# loaded_scaler = joblib.load(MODELS_DIR / 'scaler.joblib')\n",
    "\n",
    "# # Load feature list\n",
    "# with open(MODELS_DIR / 'features.txt', 'r') as f:\n",
    "#     features = f.read().strip().split('\\n')\n",
    "\n",
    "# # Prepare new data (same feature engineering as training)\n",
    "# # new_data = ...\n",
    "# # X_new = new_data[features]\n",
    "# # X_new_scaled = loaded_scaler.transform(X_new)\n",
    "\n",
    "# # Predict\n",
    "# # y_pred = loaded_model.predict(X_new_scaled)\n",
    "# # y_prob = loaded_model.predict_proba(X_new_scaled)[:, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
